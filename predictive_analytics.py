# -*- coding: utf-8 -*-
"""Predictive Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15fbsSGvd6SIURAY5wvoySYTupe8yUIHj

# Forecasting and Comparing Palm Oil, Green Coffee, and Rice Production in ASEAN Using LSTM

This notebook focuses on forecasting and comparing the production of three major agricultural commodities in Southeast Asia:  
**palm oil**, **green coffee**, and **rice**.

The objective is to analyze Indonesia’s production trends and compare them against three other key ASEAN countries: **Vietnam, Thailand, and Malaysia**.

Long Short-Term Memory (LSTM) is used to model and predict the future production trends of each commodity up to the year **2030**.

📂 Dataset used: [World Food Production Dataset on Kaggle](https://www.kaggle.com/datasets/rafsunahmad/world-food-production/data)

### 🔍 Project Workflow:
- Load and explore historical data from 1961 to 2023
- Focus on four ASEAN countries: Indonesia, Vietnam, Thailand, and Malaysia
- Visualize production trends for palm oil, green coffee, and rice
- Preprocess and normalize time-series data
- Train separate LSTM models for each commodity and country
- Evaluate model performance
- Forecast production values through 2030 and compare outcomes across countries

The final outcome is expected to provide insights for policy-makers, agricultural analysts, and investors to support long-term planning in the region’s food and agriculture sectors.

## Import Libraries
"""

## Import Libraries

# For data manipulation and numerical operations
import pandas as pd    # Used for handling structured data in tabular format
import numpy as np     # Provides support for large, multi-dimensional arrays and mathematical operations

# For data visualization
import matplotlib.pyplot as plt   # Used to plot line charts, bar charts, and more
import seaborn as sns             # Provides high-level interface for drawing attractive statistical graphics

# For building and training deep learning models (LSTM)
import tensorflow as tf
from tensorflow.keras.models import Sequential          # Linear stack of layers for the model
from tensorflow.keras.layers import LSTM, Dense         # LSTM for time series, Dense for output layer

# For feature scaling (data normalization)
from sklearn.preprocessing import MinMaxScaler          # Scales data to a specific range
from tensorflow.keras.callbacks import EarlyStopping    # callback early stopping


# For model evaluation
from sklearn.metrics import mean_squared_error          # Measures average squared difference between predictions and actual values

# Mount Google Drive to access dataset
from google.colab import drive

"""## Data Preparation

### Mount Google Drive
"""

drive.mount('/content/drive')  # Mounting Google Drive to access dataset stored in google drive.

"""### Data Loading"""

path = '/content/drive/MyDrive/Data-ML/world-food-production.csv' # Path from folder in gdrive
data = pd.read_csv(path)
data.head()

# Get column info and data types
data.info()

"""## Exploratoy Data Analysis (EDA)
This section will help understand the structure of the dataset, check for missing values, and filter only relevant countries and commodities.

### Basic Information
"""

print(f"Dataset contains {data.shape[0]} rows and {data.shape[1]} columns.")
print("\nAvailable columns:")
print(data.columns.tolist())

"""### Checking Missing Value"""

data.isnull().sum()

"""### Filter for ASEAN Countries and Selected Commodities"""

# Countries of interest
target_countries = ['Indonesia', 'Vietnam', 'Thailand', 'Malaysia']

# Selected commodities
selected_columns = [
    'Entity', 'Year',
    'Palm oil  Production (tonnes)',
    'Coffee, green Production ( tonnes)',
    'Rice  Production ( tonnes)'
]

# Filter dataset
df_filtered = data[data['Entity'].isin(target_countries)][selected_columns].copy()

# Display filtered data
df_filtered.head()

## Production summary statistics by country
df_filtered.groupby('Entity').describe(include="all")

"""### Rename Columns"""

# Clean column names by removing ' (tonnes)' from headers
df_filtered.columns = df_filtered.columns.str.replace(' \(tonnes\)', '', regex=True)
df_filtered.columns = df_filtered.columns.str.replace(' \( tonnes\)', '', regex=True)
df_filtered.columns = df_filtered.columns.str.replace('Coffee, green Production', 'Coffee green Production', regex=True)
# Preview renamed columns
df_filtered.head()

"""### Checking Outlier"""

# Check for outliers using boxplots

commodities = [
    'Palm oil  Production',
    'Coffee green Production',
    'Rice  Production'
]

# Create boxplots for each commodity
plt.figure(figsize=(15, 5))
for i, col in enumerate(commodities):
    plt.subplot(1, 3, i+1)
    sns.boxplot(x=df_filtered[col])
    plt.title(col.replace("Production", "").strip())

plt.tight_layout()
plt.show()

"""## Data Preprocessing

### Outlier Handling
Log transformation is applied to reduce the impact of extreme values in the production data of palm oil, coffee (green), and rice.
"""

# Apply log transformation to selected commodities

df_transformed = df_filtered.copy()

for col in commodities:
    df_transformed[col] = np.log1p(df_transformed[col])

df_transformed.head()

# Visualize after handling outlier
plt.figure(figsize=(15, 5))
for i, col in enumerate(commodities):
    plt.subplot(1, 3, i+1)
    sns.boxplot(x=df_transformed[col])
    plt.title(col.replace("Production", "").strip())
plt.tight_layout()
plt.show()

"""### Data Normalization
Each commodity's production values are scaled to the range [0, 1] using MinMaxScaler to normalize the value.
"""

# Create a new DataFrame to store normalized values
df_normalized = df_transformed.copy()

# Store scalers in case inverse_transform is needed later (just in case)
scalers = {}

# Apply MinMaxScaler to each commodity column
for col in commodities:
    scaler = MinMaxScaler()
    df_normalized[col] = scaler.fit_transform(df_normalized[[col]])
    scalers[col] = scaler

# Preview normalized values
df_normalized.head()

"""## Data Splitting

80% of the data is used for training, and the remaining 20% is used for testing to simulate future predictions.
"""

# List of countries and cleaned column names
target_countries = ['Indonesia', 'Vietnam', 'Thailand', 'Malaysia']
commodities = [
    'Palm oil  Production',
    'Coffee green Production',
    'Rice  Production'
]

# Dictionary to hold results
data_splits = {}

for country in target_countries:
    for commodity in commodities:
        key = f"{country} – {commodity}"

        # Filter by country and commodity
        df_sub = df_normalized[df_normalized['Entity'] == country][['Year', commodity]].dropna().reset_index(drop=True)

        if len(df_sub) > 10:
            split_idx = int(len(df_sub) * 0.8)
            train = df_sub[commodity].iloc[:split_idx].values
            test = df_sub[commodity].iloc[split_idx:].values
            test_years = df_sub['Year'].iloc[split_idx + 5:].values  # for LSTM alignment

            # Store to dictionary
            data_splits[key] = {
                'train': train,
                'test': test,
                'years_test': test_years,
                'total_years': len(df_sub)
            }

# Show summary
summary = pd.DataFrame([
    {
        'Country – Commodity': key,
        'Train Length': len(val['train']),
        'Test Length': len(val['test']),
        'Total Years': val['total_years']
    }
    for key, val in data_splits.items()
])

summary

"""## Data Reshape

The purpose of this section is to transform each country-commodity time series into a suitable format for LSTM input.

Since LSTM networks require 3D input in the format `[samples, time steps, features]`
"""

# Reshape sequences for all country-commodity pairs
look_back = 5  # Number of years used to predict the next

def create_sequences(series, look_back=5):
    X, y = [], []
    for i in range(len(series) - look_back):
        X.append(series[i:i+look_back])
        y.append(series[i+look_back])
    return np.array(X), np.array(y)

reshaped_data = {}

for key, split in data_splits.items():
    train = split['train']
    test = split['test']

    # Generate input-output sequences
    X_train, y_train = create_sequences(train, look_back)
    X_test, y_test = create_sequences(test, look_back)

    # Reshape to 3D for LSTM input
    X_train = X_train.reshape((X_train.shape[0], look_back, 1))
    X_test = X_test.reshape((X_test.shape[0], look_back, 1))

    reshaped_data[key] = {
        'X_train': X_train,
        'y_train': y_train,
        'X_test': X_test,
        'y_test': y_test,
        'years_test': split['years_test'][:len(y_test)]
    }

# Show all available keys in reshaped_data
print("Available pairs in reshaped_data:")
for key in reshaped_data.keys():
    print("-", key)

# Check one to inspect from reshaped_data
key = "Indonesia – Rice  Production"

# Print shapes of each component
print(f"\nData for {key}:")
print("X_train shape:", reshaped_data[key]['X_train'].shape)
print("y_train shape:", reshaped_data[key]['y_train'].shape)
print("X_test shape:", reshaped_data[key]['X_test'].shape)
print("y_test shape:", reshaped_data[key]['y_test'].shape)
print("years_test shape:", reshaped_data[key]['years_test'].shape)

"""## Modelling and Testing

### Modelling
An LSTM model is built for each country–commodity pair using the reshaped training data.  
Each model learns to predict the next year's production based on the past years.

The model uses:
- 1 LSTM layer with 50 units
- ReLU activation
- MSE as the loss function
- Adam optimizer
"""

def train_lstm_model(X_train, y_train, X_test, y_test, look_back=5, epochs=100):
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=(look_back, 1)))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')

    # Define EarlyStopping
    early_stop = EarlyStopping(
        monitor='loss',
        patience=10,
        restore_best_weights=True,
        mode='min',
        min_delta=0.001,
        verbose=1
    )

    # Train with callback
    model.fit(
        X_train, y_train,
        epochs=epochs,
        verbose=1,
        callbacks=[early_stop]
    )

    # Predict and evaluate
    y_pred = model.predict(X_test, verbose=0)
    mse = mean_squared_error(y_test, y_pred)
    rmse = mse ** 0.5

    return model, y_pred, mse, rmse

# Var for stored the result
model_results = {}

# Loop through all pairs and train models
for key in reshaped_data.keys():
    print(f"\n🚀 Training for {key}")
    data = reshaped_data[key]
    model, y_pred, mse, rmse = train_lstm_model(
        data['X_train'], data['y_train'],
        data['X_test'], data['y_test'],
        look_back=5, epochs=100
    )
    model_results[key] = {
        'model': model,
        'mse': mse,
        'rmse': rmse,
        'y_pred': y_pred
    }

"""### Testing: Evaluation and Visualization

Each model is evaluated using:
- **MSE (Mean Squared Error)**
- **RMSE (Root Mean Squared Error)**

The predictions are plotted against actual values for both training and testing sets.

#### Table Evaluation MSE and RMSE
"""

results_summary = []

for key in model_results:
    model = model_results[key]['model']
    data = reshaped_data[key]
    y_pred = model.predict(data['X_test'], verbose=0)
    y_true = data['y_test']

    # Get corresponding scaler
    commodity = key.split("–")[-1].strip()
    scaler = scalers[commodity]

    # Inverse transform
    y_pred_inv = scaler.inverse_transform(y_pred)
    y_true_inv = scaler.inverse_transform(y_true.reshape(-1, 1))

    # Compute errors
    mse = mean_squared_error(y_true_inv, y_pred_inv)
    rmse = mse ** 0.5

    results_summary.append({
        'Country – Commodity': key,
        'MSE': mse,
        'RMSE': rmse
    })

# Convert to DataFrame
results_df = pd.DataFrame(results_summary)
results_df

"""#### Plot Testing and Training"""

# After getting prediction
scaler_key = key.split("–")[-1].strip()  # Get only the commodity name
scaler = scalers[scaler_key]

# Inverse transform
y_pred_inv = scaler.inverse_transform(y_pred)
y_test_inv = scaler.inverse_transform(data['y_test'].reshape(-1, 1))

def plot_forecast_with_years_original(key, model, data_dict, scalers, save=False):
    data = data_dict[key]
    commodity = key.split("–")[-1].strip()
    scaler = scalers[commodity]

    # Predict on both sets
    y_train_pred = model.predict(data['X_train'], verbose=0)
    y_test_pred = model.predict(data['X_test'], verbose=0)

    # Inverse transform
    y_train_actual = scaler.inverse_transform(data['y_train'].reshape(-1, 1)).flatten()
    y_train_pred_actual = scaler.inverse_transform(y_train_pred).flatten()

    y_test_actual = scaler.inverse_transform(data['y_test'].reshape(-1, 1)).flatten()
    y_test_pred_actual = scaler.inverse_transform(y_test_pred).flatten()

    # Years
    years_test = data['years_test'][:len(y_test_actual)]
    start_year = years_test[0] - len(y_train_actual)
    years_train = list(range(start_year, years_test[0]))

    # Plot
    plt.figure(figsize=(14, 5))

    # Training
    plt.subplot(1, 2, 1)
    plt.plot(years_train, y_train_actual, label='Actual Train')
    plt.plot(years_train, y_train_pred_actual, '--', label='Predicted Train')
    plt.title(f'Training – {key}')
    plt.xlabel('Year'); plt.ylabel('Production (tonnes)')
    plt.xticks(rotation=45); plt.legend(); plt.grid(True)

    # Testing
    plt.subplot(1, 2, 2)
    plt.plot(years_test, y_test_actual, label='Actual Test')
    plt.plot(years_test, y_test_pred_actual, '--', label='Predicted Test')
    plt.title(f'Testing – {key}')
    plt.xlabel('Year'); plt.ylabel('Production (tonnes)')
    plt.xticks(rotation=45); plt.legend(); plt.grid(True)

    plt.tight_layout()
    if save:
        safe_key = key.replace(" ", "_").replace(",", "").replace("–", "-")
        plt.savefig(f"{safe_key}.png")
    plt.show()

# Loop through all country–commodity pairs and plot predictions with inverse transform
for key in model_results:
    print(f"\n📊 Visualizing forecast for {key}")
    plot_forecast_with_years_original(
        key=key,
        model=model_results[key]['model'],
        data_dict=reshaped_data,
        scalers=scalers,
    )

"""## Future Prediction"""

forecast_results = []

look_back = 5
target_year = 2030

for key in model_results:
    model = model_results[key]['model']
    data = reshaped_data[key]
    scaler = scalers[key.split("–")[-1].strip()]
    commodity = key.split("–")[-1].strip()
    country = key.split("–")[0].strip()

    last_sequence = np.concatenate([data['y_train'], data['y_test']])[-look_back:]
    years_last_known = data['years_test'][-1]
    years_to_forecast = target_year - years_last_known

    current_sequence = last_sequence.copy()
    preds_scaled = []

    for _ in range(years_to_forecast):
        input_seq = current_sequence[-look_back:].reshape((1, look_back, 1))
        next_pred = model.predict(input_seq, verbose=0)
        preds_scaled.append(next_pred[0, 0])
        current_sequence = np.append(current_sequence, next_pred[0, 0])

    # Inverse transform to original scale
    preds_actual = scaler.inverse_transform(np.array(preds_scaled).reshape(-1, 1)).flatten()

    forecast_results.append({
        'Country': country,
        'Commodity': commodity,
        'Forecast2030': preds_actual[-1],
        'FromYear': years_last_known + 1,
        'ToYear': target_year
    })

forecast_df = pd.DataFrame(forecast_results)
forecast_df.sort_values(by=['Commodity', 'Forecast2030'], ascending=[True, False])

sns.set(style="whitegrid")
plt.figure(figsize=(12, 6))

# Barchart for comparison Indonesia and Others
sns.barplot(data=forecast_df, x='Commodity', y='Forecast2030', hue='Country')

plt.title('Forecasted Production in 2030 by Commodity (Indonesia vs Others)')
plt.ylabel('Production (tonnes)')
plt.xlabel('Commodity')
plt.xticks(rotation=15)
plt.legend(title='Country')
plt.tight_layout()
plt.show()

def forecast_future(model, last_sequence, n_years, scaler, look_back=5):
    predictions = []
    current_sequence = last_sequence.copy()

    for _ in range(n_years):
        input_seq = current_sequence[-look_back:].reshape((1, look_back, 1))
        next_pred = model.predict(input_seq, verbose=0)
        predictions.append(next_pred[0, 0])
        current_sequence = np.append(current_sequence, next_pred[0, 0])

    # Inverse transform
    predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()
    return predictions

# Create dictionary to group predictions per commodity
forecast_by_commodity = {}

for key in model_results:
    model = model_results[key]['model']
    data = reshaped_data[key]
    scaler = scalers[key.split("–")[-1].strip()]
    commodity = key.split("–")[-1].strip()
    country = key.split("–")[0].strip()

    # Last known sequence and years
    last_sequence = np.concatenate([data['y_train'], data['y_test']])[-5:]
    years_last_known = data['years_test'][-1]
    years_to_forecast = 2030 - years_last_known
    forecast_years = list(range(years_last_known + 1, 2031))

    # Forecast using your function
    preds = forecast_future(model, last_sequence, years_to_forecast, scaler)

    # Save to dictionary
    if commodity not in forecast_by_commodity:
        forecast_by_commodity[commodity] = {}

    forecast_by_commodity[commodity][country] = {
        'years': forecast_years,
        'predictions': preds
    }

# Plot forecast per commodity
for commodity, countries_data in forecast_by_commodity.items():
    plt.figure(figsize=(10, 5))
    for country, values in countries_data.items():
        plt.plot(values['years'], values['predictions'], marker='o', label=country)

    plt.title(f'Forecast Comparison (Until 2030) – {commodity}')
    plt.xlabel('Year'); plt.ylabel('Production (tonnes)')
    plt.grid(True); plt.legend(title='Country')
    plt.tight_layout()
    plt.show()